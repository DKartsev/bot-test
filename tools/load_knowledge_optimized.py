#!/usr/bin/env python3
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∑–Ω–∞–Ω–∏–π –≤ Supabase
"""

import os
import sys
import psycopg2
import uuid
from datetime import datetime
from pathlib import Path
import re
import json

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö Supabase
DB_CONFIG = {
    'host': 'aws-0-eu-north-1.pooler.supabase.com',
    'port': 5432,
    'database': 'postgres',
    'user': 'postgres.ymfduihrjjuzwuckbjjh',
    'password': 'mn4c0Je402fgh3mc5'
}

def parse_markdown_file(file_path: Path) -> dict:
    """–ü–∞—Ä—Å–∏—Ç markdown —Ñ–∞–π–ª –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
    content = file_path.read_text(encoding='utf-8')
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º frontmatter (–º–µ–∂–¥—É ---)
    frontmatter_match = re.match(r'^---\n(.*?)\n---\n', content, re.DOTALL)
    
    if frontmatter_match:
        frontmatter_text = frontmatter_match.group(1)
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ YAML
        metadata = {}
        for line in frontmatter_text.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip().strip('"')
                if key == 'tags':
                    # –ü–∞—Ä—Å–∏–º —Ç–µ–≥–∏
                    tags_text = value.strip('[]')
                    tags = [tag.strip().strip('"') for tag in tags_text.split(',') if tag.strip()]
                    metadata[key] = tags
                else:
                    metadata[key] = value
        
        # –£–±–∏—Ä–∞–µ–º frontmatter –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        body_md = content[frontmatter_match.end():].strip()
    else:
        # –ï—Å–ª–∏ –Ω–µ—Ç frontmatter, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        metadata = {
            'title': file_path.stem.replace('-', ' ').title(),
            'slug': file_path.stem,
            'tags': []
        }
        body_md = content
    
    return {
        'title': metadata.get('title', file_path.stem.replace('-', ' ').title()),
        'slug': metadata.get('slug', file_path.stem),
        'tags': metadata.get('tags', []),
        'body_md': body_md
    }

def create_kb_article(cursor, article_data: dict) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç–∞—Ç—å—é –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
    article_id = str(uuid.uuid4())
    
    cursor.execute("""
        INSERT INTO kb_articles (id, title, slug, body_md, tags, updated_at)
        VALUES (%s, %s, %s, %s, %s, %s)
        ON CONFLICT (slug) DO UPDATE SET
            title = EXCLUDED.title,
            body_md = EXCLUDED.body_md,
            tags = EXCLUDED.tags,
            updated_at = EXCLUDED.updated_at
        RETURNING id
    """, (
        article_id,
        article_data['title'],
        article_data['slug'],
        article_data['body_md'],
        json.dumps(article_data['tags']),  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JSON
        datetime.now()
    ))
    
    return cursor.fetchone()[0]

def create_chunks_from_article(cursor, article_id: str, article_data: dict):
    """–°–æ–∑–¥–∞–µ—Ç —á–∞–Ω–∫–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã"""
    content = article_data['body_md']
    
    # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
    paragraphs = content.split('\n\n')
    chunks = []
    
    for i, paragraph in enumerate(paragraphs):
        if paragraph.strip():
            chunk_text = paragraph.strip()
            if len(chunk_text) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞
                chunks.append({
                    'text': chunk_text,
                    'index': i
                })
    
    # –í—Å—Ç–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏
    for chunk in chunks:
        chunk_id = str(uuid.uuid4())
        cursor.execute("""
            INSERT INTO kb_chunks (id, article_id, chunk_text, chunk_index, created_at)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (article_id, chunk_index) DO UPDATE SET
                chunk_text = EXCLUDED.chunk_text,
                created_at = EXCLUDED.created_at
        """, (
            chunk_id,
            article_id,
            chunk['text'],
            chunk['index'],
            datetime.now()
        ))

def check_existing_data(cursor) -> dict:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ –±–∞–∑–µ"""
    stats = {}
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—å–∏
    cursor.execute("SELECT COUNT(*) FROM kb_articles")
    stats['articles'] = cursor.fetchone()[0]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞–Ω–∫–∏
    cursor.execute("SELECT COUNT(*) FROM kb_chunks")
    stats['chunks'] = cursor.fetchone()[0]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞–Ω–∫–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
    cursor.execute("SELECT COUNT(*) FROM kb_chunks WHERE embedding IS NOT NULL")
    stats['chunks_with_embeddings'] = cursor.fetchone()[0]
    
    return stats

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∑–Ω–∞–Ω–∏–π –≤ Supabase")
    
    # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ñ–∞–π–ª–∞–º–∏ –∑–Ω–∞–Ω–∏–π
    kb_path = Path('apps/support-gateway/kb_articles')
    
    if not kb_path.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ {kb_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        print("üìÅ –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –ø–∞–ø–∫—É –∏ —Ñ–∞–π–ª
        kb_path.mkdir(parents=True, exist_ok=True)
        test_file = kb_path / "test-article.md"
        test_file.write_text("""---
title: "–¢–µ—Å—Ç–æ–≤–∞—è —Å—Ç–∞—Ç—å—è"
slug: "test-article"
tags: ["—Ç–µ—Å—Ç", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è"]
---

# –¢–µ—Å—Ç–æ–≤–∞—è —Å—Ç–∞—Ç—å—è

–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–∞—è —Å—Ç–∞—Ç—å—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã –∑–∞–≥—Ä—É–∑–∫–∏ –∑–Ω–∞–Ω–∏–π.

## –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã

1. **–í–≤–µ–¥–µ–Ω–∏–µ** - –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
2. **–ü—Ä–∞–∫—Ç–∏–∫–∞** - –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
3. **–ó–∞–∫–ª—é—á–µ–Ω–∏–µ** - –∏—Ç–æ–≥–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

–≠—Ç–∞ —Å—Ç–∞—Ç—å—è —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å–∏—Å—Ç–µ–º—ã.
""")
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª: {test_file}")
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cursor = conn.cursor()
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Supabase: {e}")
        sys.exit(1)
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        print("\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        existing_stats = check_existing_data(cursor)
        print(f"  - –°—Ç–∞—Ç–µ–π: {existing_stats['articles']}")
        print(f"  - –ß–∞–Ω–∫–æ–≤: {existing_stats['chunks']}")
        print(f"  - –ß–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {existing_stats['chunks_with_embeddings']}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ markdown —Ñ–∞–π–ª–æ–≤
        md_files = list(kb_path.glob('*.md'))
        print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ {len(md_files)} markdown —Ñ–∞–π–ª–æ–≤")
        
        if not md_files:
            print("‚ùå Markdown —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
        loaded_count = 0
        chunks_created = 0
        
        for md_file in md_files:
            try:
                print(f"\nüìñ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {md_file.name}...")
                
                # –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª
                article_data = parse_markdown_file(md_file)
                
                # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç—å—é –≤ –±–∞–∑–µ
                article_id = create_kb_article(cursor, article_data)
                print(f"  ‚úÖ –°—Ç–∞—Ç—å—è '{article_data['title']}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (ID: {article_id})")
                
                # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
                create_chunks_from_article(cursor, article_id, article_data)
                print(f"  üìù –ß–∞–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è —Å—Ç–∞—Ç—å–∏")
                
                loaded_count += 1
                
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {md_file.name}: {e}")
                continue
        
        # –ö–æ–º–º–∏—Ç–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        conn.commit()
        print(f"\nüéâ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {loaded_count}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        final_stats = check_existing_data(cursor)
        print(f"\nüìä –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  - –í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π: {final_stats['articles']}")
        print(f"  - –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {final_stats['chunks']}")
        print(f"  - –ß–∞–Ω–∫–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {final_stats['chunks_with_embeddings']}")
        
        if final_stats['chunks_with_embeddings'] == 0:
            print(f"\n‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: –ß–∞–Ω–∫–∏ —Å–æ–∑–¥–∞–Ω—ã, –Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
            print(f"   –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω—É–∂–µ–Ω OpenAI API –∫–ª—é—á")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()
        print("üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Supabase –∑–∞–∫—Ä—ã—Ç–æ")

if __name__ == "__main__":
    main()
